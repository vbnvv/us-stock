import requests
from bs4 import BeautifulSoup
import csv
import os
from urllib.parse import urlparse, parse_qs
import re
import json
from datetime import datetime
import pandas as pd

class StockComparison:
    def __init__(self):
        self.script_dir = os.path.dirname(os.path.abspath(__file__))
        self.project_root = os.path.dirname(self.script_dir)
        self.data_dir = os.path.join(self.project_root, 'data', 'all_bank_stock')
        self.log_dir = os.path.join(self.project_root, 'data', 'logs')
        os.makedirs(self.log_dir, exist_ok=True)
        
    def load_old_data(self, bank_name):
        """è¼‰å…¥èˆŠçš„éŠ€è¡Œè‚¡ç¥¨æ•¸æ“š"""
        old_file = os.path.join(self.data_dir, f'{bank_name}_bank.csv')
        old_stocks = set()
        
        if os.path.exists(old_file):
            try:
                with open(old_file, 'r', encoding='utf-8-sig') as f:
                    reader = csv.DictReader(f)
                    for row in reader:
                        ticker = row.get('ä»£ç¢¼', '').strip()
                        if ticker:
                            old_stocks.add(ticker)
                
            except Exception as e:
                print(f"è¼‰å…¥ {bank_name} èˆŠæ•¸æ“šå¤±æ•—: {e}")
        else:
            print(f"æ‰¾ä¸åˆ° {bank_name} èˆŠæ•¸æ“šæª”æ¡ˆ")
        
        return old_stocks
    
    def compare_stocks(self, bank_name, new_stocks):
        """æ¯”å°æ–°èˆŠè‚¡ç¥¨æ¸…å–®"""
        old_stocks = self.load_old_data(bank_name)
        
        # æ‰¾å‡ºæ–°å¢å’Œç§»é™¤çš„è‚¡ç¥¨
        added_stocks = new_stocks - old_stocks
        removed_stocks = old_stocks - new_stocks
        
        return {
            'bank_name': bank_name,
            'old_count': len(old_stocks),
            'new_count': len(new_stocks),
            'added': list(added_stocks),
            'removed': list(removed_stocks),
            'total_change': len(added_stocks) - len(removed_stocks)
        }
    
    def save_comparison_log(self, comparisons):
        """å„²å­˜æ¯”å°è¨˜éŒ„åˆ°list_change.csvæª”æ¡ˆ"""
        log_file = os.path.join(self.log_dir, 'list_change.csv')
        
        # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡å»ºç«‹æ¨™é¡Œè¡Œ
        file_exists = os.path.exists(log_file)
        
        # æº–å‚™è¨˜éŒ„æ•¸æ“š
        log_data = []
        for comp in comparisons:
            # æ–°å¢è‚¡ç¥¨è¨˜éŒ„
            for ticker in comp['added']:
                log_data.append({
                    'æ—¥æœŸ': datetime.now().strftime('%Y-%m-%d'),
                    'éŠ€è¡Œ': comp['bank_name'],
                    'è‚¡ç¥¨ä»£ç¢¼': ticker,
                    'è®Šæ›´é¡å‹': 'ä¸Šæ¶',
                    'èˆŠæ•¸é‡': comp['old_count'],
                    'æ–°æ•¸é‡': comp['new_count'],
                    'æ·¨è®ŠåŒ–': comp['total_change']
                })
            
            # ç§»é™¤è‚¡ç¥¨è¨˜éŒ„
            for ticker in comp['removed']:
                log_data.append({
                    'æ—¥æœŸ': datetime.now().strftime('%Y-%m-%d'),
                    'éŠ€è¡Œ': comp['bank_name'],
                    'è‚¡ç¥¨ä»£ç¢¼': ticker,
                    'è®Šæ›´é¡å‹': 'ä¸‹æ¶',
                    'èˆŠæ•¸é‡': comp['old_count'],
                    'æ–°æ•¸é‡': comp['new_count'],
                    'æ·¨è®ŠåŒ–': comp['total_change']
                })
            
            # å¦‚æœæ²’æœ‰è®Šæ›´ï¼Œä¸è¨˜éŒ„åˆ°æª”æ¡ˆä¸­
            # if not comp['added'] and not comp['removed']:
            #     log_data.append({
            #         'æ—¥æœŸ': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            #         'éŠ€è¡Œ': comp['bank_name'],
            #         'è‚¡ç¥¨ä»£ç¢¼': 'ç„¡è®Šæ›´',
            #         'è®Šæ›´é¡å‹': 'ç„¡è®Šæ›´',
            #         'èˆŠæ•¸é‡': comp['old_count'],
            #         'æ–°æ•¸é‡': comp['new_count'],
            #         'æ·¨è®ŠåŒ–': comp['total_change']
            #     })
        
        # å„²å­˜è¨˜éŒ„æª” - æ¯æ¬¡æ–°å¢åˆ°æª”æ¡ˆæœ«å°¾
        if log_data:
            with open(log_file, 'a', newline='', encoding='utf-8-sig') as f:
                fieldnames = ['æ—¥æœŸ', 'éŠ€è¡Œ', 'è‚¡ç¥¨ä»£ç¢¼', 'è®Šæ›´é¡å‹', 'èˆŠæ•¸é‡', 'æ–°æ•¸é‡', 'æ·¨è®ŠåŒ–']
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                
                # å¦‚æœæª”æ¡ˆä¸å­˜åœ¨ï¼Œå¯«å…¥æ¨™é¡Œè¡Œ
                if not file_exists:
                    writer.writeheader()
                
                writer.writerows(log_data)
            
            print(f"\nâœ… è®Šæ›´è¨˜éŒ„å·²æ–°å¢è‡³: {log_file}")
        else:
            print("\nâŒ æ²’æœ‰ç™¼ç¾ä»»ä½•è®Šæ›´")
        
        return log_file
    

# å…¨åŸŸè®Šæ•¸ä¾†å„²å­˜æ¯”å°çµæœ
stock_comparison = StockComparison()

def esun_bank_stock():
    """ç‰å±±éŠ€è¡Œè‚¡ç¥¨çˆ¬èŸ²"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)
    output_dir = os.path.join(project_root, 'data', 'all_bank_stock')
    os.makedirs(output_dir, exist_ok=True)
    output_filename = os.path.join(output_dir, 'esun_bank.csv')
    
    # å„²å­˜æ–°çˆ¬å–çš„è‚¡ç¥¨ä»£ç¢¼
    new_stocks = set()

    url = "https://wealth.esunbank.com.tw/usstock/esun/rank9001.xdjhtm"

    try:
        response = requests.get(url)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"ç²å–ç‰å±±éŠ€è¡Œç¶²å€æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return

    soup = BeautifulSoup(response.content, 'html.parser', from_encoding='ms950')
    data_table = soup.find('table', id='oMainTable')

    if not data_table:
        print("åœ¨ç‰å±±éŠ€è¡Œé é¢ä¸Šæ‰¾ä¸åˆ°è³‡æ–™è¡¨æ ¼ã€‚")
        return


    try:
        with open(output_filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
            csv_writer = csv.writer(csvfile)
            csv_writer.writerow(['è‚¡ç¥¨åç¨±', 'ä»£ç¢¼'])

            thead = data_table.find('thead')
            original_headers = [th.get_text(strip=True) for th in thead.find_all('th')] if thead else []
            
            stock_name_index = -1
            if original_headers:
                try:
                    stock_name_index = original_headers.index('è‚¡ç¥¨åç¨±')
                except ValueError:
                    print("åœ¨è¡¨æ ¼ä¸­æ‰¾ä¸åˆ° 'è‚¡ç¥¨åç¨±' æ¨™é¡Œã€‚")
                    return
            else:
                print("æ‰¾ä¸åˆ°è¡¨æ ¼æ¨™é¡Œã€‚")
                return

            tbody = data_table.find('tbody')
            if tbody:
                for row in tbody.find_all('tr'):
                    cells = row.find_all('td')
                    if not cells or len(cells) <= stock_name_index:
                        continue

                    stock_name = ''
                    ticker = ''

                    stock_name_cell = cells[stock_name_index]
                    stock_name = stock_name_cell.get_text(strip=True)

                    a_tag = stock_name_cell.find('a')
                    if a_tag and 'href' in a_tag.attrs:
                        relative_link = a_tag['href']
                        try:
                            parsed_url = urlparse(relative_link)
                            query_params = parse_qs(parsed_url.query)
                            if 'a' in query_params:
                                ticker = query_params['a'][0]
                        except Exception:
                            ticker = ''

                    if stock_name or ticker:
                        csv_writer.writerow([stock_name, ticker])
                        if ticker:
                            new_stocks.add(ticker)

        return new_stocks

    except Exception as e:
        print(f"è§£ææˆ–å¯«å…¥ç‰å±±éŠ€è¡Œæª”æ¡ˆæ™‚ç™¼ç”Ÿæ„å¤–éŒ¯èª¤: {e}")
        return set()

def mega_bank_stock():
    """å…†è±éŠ€è¡Œè‚¡ç¥¨çˆ¬èŸ²"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)
    output_dir = os.path.join(project_root, 'data', 'all_bank_stock')
    os.makedirs(output_dir, exist_ok=True)
    output_filename = os.path.join(output_dir, 'mega_bank.csv')
    
    # å„²å­˜æ–°çˆ¬å–çš„è‚¡ç¥¨ä»£ç¢¼
    new_stocks = set()

    url = "https://fund.megabank.com.tw/w/stockindexrank.djhtm?a=us&b=1000&c=desc"

    try:
        response = requests.get(url)
        response.raise_for_status()
    except requests.exceptions.RequestException as e:
        print(f"ç²å–å…†è±éŠ€è¡Œç¶²å€æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return

    soup = BeautifulSoup(response.content, 'html.parser', from_encoding='ms950')
    data_table = soup.find('table', class_='datalist')

    if not data_table:
        print("åœ¨å…†è±éŠ€è¡Œé é¢ä¸Šæ‰¾ä¸åˆ°è³‡æ–™è¡¨æ ¼ã€‚")
        return

    try:
        with open(output_filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
            csv_writer = csv.writer(csvfile)
            headers = ['è‚¡ç¥¨åç¨±', 'ä»£ç¢¼']
            csv_writer.writerow(headers)
            
            data_rows = data_table.find_all('tr')

            if len(data_rows) > 1:
                for row in data_rows[1:]:
                    cells = [cell.get_text(strip=True) for cell in row.find_all('td')]
                    if len(cells) >= 2:
                        ticker = cells[0]
                        name = cells[1]
                        csv_writer.writerow([name, ticker])
                        if ticker:
                            new_stocks.add(ticker)

        return new_stocks

    except Exception as e:
        print(f"è§£ææˆ–å¯«å…¥å…†è±éŠ€è¡Œæª”æ¡ˆæ™‚ç™¼ç”Ÿæ„å¤–éŒ¯èª¤: {e}")
        return set()

def CTBC_bank_stock():
    """ä¸­ä¿¡éŠ€è¡Œè‚¡ç¥¨çˆ¬èŸ²"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)
    output_dir = os.path.join(project_root, 'data', 'all_bank_stock')
    os.makedirs(output_dir, exist_ok=True)
    output_filename = os.path.join(output_dir, 'CTBC_bank.csv')
    
    # å„²å­˜æ–°çˆ¬å–çš„è‚¡ç¥¨ä»£ç¢¼
    new_stocks = set()
    
    base_url = "https://ctbcbank.moneydj.com/usstock/ctbcbank/xdjjson/restalphalist.xdjjson?a={}"
    
    try:
        with open(output_filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
            csv_writer = csv.writer(csvfile)
            csv_writer.writerow(['è‚¡ç¥¨åç¨±', 'ä»£ç¢¼'])

            # éæ­· A-Z å­—æ¯
            for letter in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ':
                url = base_url.format(letter)
                
                try:
                    response = requests.get(url)
                    response.raise_for_status()
                    data = response.json()
                    
                    if data and isinstance(data, list):
                        for item in data:
                            ticker = item.get('id', '').strip()
                            stock_name = item.get('name', '').strip()
                            
                            if stock_name and ticker:
                                csv_writer.writerow([stock_name, ticker])
                                new_stocks.add(ticker)
                        
                    else:
                        print(f"å­—æ¯ {letter} æ²’æœ‰è³‡æ–™")
                        
                except requests.exceptions.RequestException as e:
                    print(f"ç²å–ä¸­ä¿¡éŠ€è¡Œå­—æ¯ {letter} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                except json.JSONDecodeError as e:
                    print(f"è§£æä¸­ä¿¡éŠ€è¡Œå­—æ¯ {letter} JSON æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                except Exception as e:
                    print(f"è™•ç†ä¸­ä¿¡éŠ€è¡Œå­—æ¯ {letter} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        return new_stocks

    except Exception as e:
        print(f"çˆ¬å–ä¸­ä¿¡éŠ€è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return set()

def tashin_bank_stock():
    """å°æ–°éŠ€è¡Œè‚¡ç¥¨çˆ¬èŸ²"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)
    output_dir = os.path.join(project_root, 'data', 'all_bank_stock')
    os.makedirs(output_dir, exist_ok=True)
    output_filename = os.path.join(output_dir, 'tashin_bank.csv')
    
    # å„²å­˜æ–°çˆ¬å–çš„è‚¡ç¥¨ä»£ç¢¼
    new_stocks = set()
    
    base_url = "https://taishinbankrwd.moneydj.com/w/html/ForeignStockPage.djhtm"
    
    try:
        with open(output_filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
            csv_writer = csv.writer(csvfile)
            csv_writer.writerow(['è‚¡ç¥¨åç¨±', 'ä»£ç¢¼'])

            page = 1
            while True:
                url = f"{base_url}?a=ALL&Page={page}"

                try:
                    response = requests.get(url)
                    response.raise_for_status()
                except requests.exceptions.RequestException as e:
                    print(f"ç„¡æ³•ç²å–ç¬¬ {page} é ã€‚éŒ¯èª¤: {e}ã€‚åœæ­¢ã€‚")
                    break
                
                # æ ¹æ“šä¹‹å‰é¡ä¼¼ç¶²ç«™çš„ç¶“é©—ä½¿ç”¨ 'ms950' ç·¨ç¢¼
                soup = BeautifulSoup(response.content, 'html.parser', from_encoding='ms950')
                
                # ä¸»è¦è¡¨æ ¼ä¼¼ä¹æ˜¯ç¬¬ä¸€å€‹å…·æœ‰ 'djclass_table' é¡åˆ¥çš„è¡¨æ ¼æˆ–é€šç”¨è¡¨æ ¼
                data_table = soup.find('table', {'class': 'djclass_table'})
                if not data_table:
                    data_table = soup.find('table') # å‚™ç”¨æ–¹æ¡ˆï¼šç¬¬ä¸€å€‹è¡¨æ ¼

                if not data_table:
                    print(f"åœ¨ç¬¬ {page} é æ‰¾ä¸åˆ°è³‡æ–™è¡¨æ ¼ã€‚çˆ¬å–å®Œæˆã€‚")
                    break
                
                rows = data_table.find_all('tr')
                
                # å¦‚æœåªæœ‰æ¨™é¡Œåˆ—å­˜åœ¨ï¼Œæˆ‘å€‘å°±å®Œæˆäº†
                if len(rows) <= 1:
                    break

                rows_processed = 0
                for row in rows[1:]: # è·³éæ¨™é¡Œ
                    cells = row.find_all('td')
                    if len(cells) > 2:
                        try:
                            # è‚¡ç¥¨åç¨±åœ¨ç¬¬äºŒæ¬„ï¼Œä»£ç¢¼åœ¨ç¬¬ä¸‰æ¬„
                            stock_name = cells[1].get_text(strip=True)
                            ticker = cells[3].get_text(strip=True) #äº¤æ˜“æ‰€ä»£ç¢¼
                            
                            stock_name = re.sub(r'ï¼œ[^ï¼]+ï¼', '', stock_name)
                            stock_name = re.sub(r'ï¼ˆ[^ï¼‰]+ï¼‰', '', stock_name).strip()

                            if stock_name and ticker:
                                csv_writer.writerow([stock_name, ticker])
                                new_stocks.add(ticker)
                                rows_processed += 1
                        except IndexError:
                            continue # è·³éæ ¼å¼éŒ¯èª¤çš„åˆ—
                
                if rows_processed == 0:
                    print(f"åœ¨ç¬¬ {page} é æ²’æœ‰è™•ç†åˆ°æœ‰æ•ˆè³‡æ–™ã€‚çˆ¬å–å®Œæˆã€‚")
                    break

                page += 1

        return new_stocks

    except Exception as e:
        print(f"çˆ¬å–å°æ–°éŠ€è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return set()

def ubot_bank_stock():
    """UBOT éŠ€è¡Œè‚¡ç¥¨çˆ¬èŸ²"""
    script_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(script_dir)
    output_dir = os.path.join(project_root, 'data', 'all_bank_stock')
    os.makedirs(output_dir, exist_ok=True)
    output_filename = os.path.join(output_dir, 'ubot_bank.csv')
    
    # å„²å­˜æ–°çˆ¬å–çš„è‚¡ç¥¨ä»£ç¢¼
    new_stocks = set()
    
    url = "https://www.yesfund.com.tw/w/djjson/overseasRankListJosn.djjson?A=1&B=0"

    try:
        response = requests.get(url)
        response.raise_for_status()
        data = response.json()
    except requests.exceptions.RequestException as e:
        print(f"ç²å– UBOT ç¶²å€æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return
    except json.JSONDecodeError:
        print("è§£æ UBOT å›æ‡‰ä¸­çš„ JSON æ™‚ç™¼ç”ŸéŒ¯èª¤ã€‚")
        return

    try:
        results = data.get("ResultSet", {}).get("Result", [])
        if not results:
            print("åœ¨ UBOT çš„ JSON è³‡æ–™ä¸­æ‰¾ä¸åˆ°çµæœã€‚")
            return

        with open(output_filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
            csv_writer = csv.writer(csvfile)
            csv_writer.writerow(['è‚¡ç¥¨åç¨±', 'ä»£ç¢¼'])

            for item in results:
                stock_name = item.get("V2", "").strip()
                ticker = item.get("V3", "").strip()
                
                stock_name = re.sub(r'ï¼œ[^ï¼]+ï¼', '', stock_name)
                stock_name = re.sub(r'ï¼ˆ[^ï¼‰]+ï¼‰', '', stock_name).strip()

                if stock_name and ticker:
                    csv_writer.writerow([stock_name, ticker])
                    new_stocks.add(ticker)
        
        return new_stocks

    except Exception as e:
        print(f"çˆ¬å– UBOT éŠ€è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return set()

def main():
    """ä¸»ç¨‹å¼"""
    print("ğŸš€ é–‹å§‹åŸ·è¡Œæ‰€æœ‰éŠ€è¡Œçš„è‚¡ç¥¨çˆ¬å–...")
    
    # å„²å­˜æ‰€æœ‰æ¯”å°çµæœ
    comparison_results = []
    
    # åŸ·è¡Œå„éŠ€è¡Œçˆ¬èŸ²ä¸¦æ”¶é›†çµæœ
    banks = [
        ("esun", esun_bank_stock, "ç‰å±±éŠ€è¡Œ"),
        ("mega", mega_bank_stock, "å…†è±éŠ€è¡Œ"),
        ("CTBC", CTBC_bank_stock, "ä¸­ä¿¡éŠ€è¡Œ"),
        #("tashin", tashin_bank_stock, "å°æ–°éŠ€è¡Œ"),
        ("ubot", ubot_bank_stock, "UBOT éŠ€è¡Œ")
    ]
    
    for bank_code, bank_func, bank_name in banks:
        print(f"\nğŸ¦ æ­£åœ¨è™•ç† {bank_name}...")
        
        # 1. å…ˆè®€å–èˆŠçš„CSV (ç”¨ç¨ç«‹çš„è®Šæ•¸)
        old_stocks = stock_comparison.load_old_data(bank_code)
        print(f"   èˆŠè³‡æ–™: {len(old_stocks)} æª”")
        
        # 2. é–‹å§‹çˆ¬èŸ²ä¸¦å­˜å…¥CSV
        new_stocks = bank_func()
        print(f"   æ–°è³‡æ–™: {len(new_stocks)} æª”")
        
        if new_stocks:
            # 3. æ¯”å°èˆŠçš„CSV(é€™è£¡æ˜¯è®Šæ•¸)æ¯”è¼ƒæ–°çˆ¬èŸ²çš„å…§å®¹
            added_stocks = new_stocks - old_stocks
            removed_stocks = old_stocks - new_stocks
            
            comparison = {
                'bank_name': bank_code,
                'old_count': len(old_stocks),
                'new_count': len(new_stocks),
                'added': list(added_stocks),
                'removed': list(removed_stocks),
                'total_change': len(added_stocks) - len(removed_stocks)
            }
            comparison_results.append(comparison)
            print(f"   {bank_name}æ¯”å°å®Œæˆ: æ–°å¢ {len(comparison['added'])} æª”, ç§»é™¤ {len(comparison['removed'])} æª”")

    # 4. ç”¢ç”Ÿå·®ç•°å¦å­˜ä¸€å€‹CSVç´€éŒ„ä¸Šä¸‹æ¶
    if comparison_results:
        stock_comparison.save_comparison_log(comparison_results)
    else:
        print("âŒ æ²’æœ‰æ¯”å°çµæœå¯å„²å­˜")
    

if __name__ == "__main__":
    main() 